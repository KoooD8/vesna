#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π Web Research Agent

–í–ù–ò–ú–ê–ù–ò–ï: experimental/legacy.
–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏ –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω:
- –°–æ–¥–µ—Ä–∂–∏—Ç HTML-—Å–∫—Ä–µ–π–ø–∏–Ω–≥ Google (—Ö—Ä—É–ø–∫–æ, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –Ω–∞—Ä—É—à–∞–µ—Ç ToS).
- –õ–æ–≥–∏–∫–∞ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è —Å WorkingWebAgent –∏ –º–æ–∂–µ—Ç —Ä–∞—Å—Ö–æ–¥–∏—Ç—å—Å—è.
–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å WorkingWebAgent –∏–∑ agents.web_research.
"""

import requests
import json
from datetime import datetime
import re
from urllib.parse import quote_plus
from bs4 import BeautifulSoup

class WebResearchAgent:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def search_reddit(self, query):
        """–ü–æ–∏—Å–∫ –Ω–∞ Reddit"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            keywords = self.extract_keywords(query)
            search_query = ' '.join(keywords)
            
            print(f"üîç –ü–æ–∏—Å–∫ –Ω–∞ Reddit: '{search_query}'")
            
            url = "https://www.reddit.com/search.json"
            params = {
                'q': search_query,
                'limit': 10,
                'sort': 'relevance',  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ relevance –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                't': 'month'  # –†–∞—Å—à–∏—Ä–∏–ª–∏ –¥–æ –º–µ—Å—è—Ü–∞
            }
            
            response = self.session.get(url, params=params, timeout=10)
            if response.status_code != 200:
                return [{'error': f'HTTP {response.status_code}: {response.text}'}]
            
            data = response.json()
            
            results = []
            for post in data.get('data', {}).get('children', []):
                post_data = post.get('data', {})
                title = post_data.get('title', '')
                selftext = post_data.get('selftext', '')
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if self.is_relevant(title + ' ' + selftext, keywords):
                    results.append({
                        'title': title,
                        'subreddit': post_data.get('subreddit', ''),
                        'score': post_data.get('score', 0),
                        'url': f"https://reddit.com{post_data.get('permalink', '')}",
                        'author': post_data.get('author', ''),
                        'text': selftext[:300] if selftext else '',
                        'created': post_data.get('created_utc', 0)
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:5]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-5
            
        except Exception as e:
            return [{'error': f'–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Reddit: {str(e)}'}]
    
    def extract_keywords(self, query):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç—å
        stop_words = {
            '–Ω–∞–π–¥–∏', '–Ω–∞–π—Ç–∏', '–ø–æ–∏—Å–∫', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é',
            '–ø—Ä–æ', '–æ', '–∏', '—Å', '–∫–æ–≥–¥–∞', '–∫–∞–∫', '—á—Ç–æ', '—Ç–∞–º',
            'reddit', 'find', 'search', 'information', 'about', 'on', 'with'
        }
        
        # –û—á–∏—â–∞–µ–º –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'\b\w+\b', query.lower())
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        # –ï—Å–ª–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –±–µ—Ä–µ–º –≤—Å–µ
        if not keywords:
            keywords = [word for word in words if len(word) > 1]
        
        return keywords[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def is_relevant(self, text, keywords):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        if not keywords:
            return True
        
        text_lower = text.lower()
        matches = sum(1 for keyword in keywords if keyword in text_lower)
        
        # –°—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ —Ö–æ—Ç—è –±—ã 30% –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        return matches >= max(1, len(keywords) * 0.3)
    
    def search_general(self, query):
        """–û–±—â–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ä–∞–∑–Ω—ã–º —Å—É–±—Ä–µ–¥–¥–∏—Ç–∞–º"""
        try:
            keywords = self.extract_keywords(query)
            search_query = ' '.join(keywords) if keywords else query
            
            print(f"üåê –û–±—â–∏–π –ø–æ–∏—Å–∫: '{search_query}'")
            print(f"üîë –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords}")
            
            # –ü–æ–∏—Å–∫ –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å—É–±—Ä–µ–¥–¥–∏—Ç–∞—Ö
            subreddits = ['OpenAI', 'MachineLearning', 'artificial', 'technology', 'singularity']
            all_results = []
            
            for subreddit in subreddits:
                print(f"üîç –ü–æ–∏—Å–∫ –≤ r/{subreddit}...")
                url = f"https://www.reddit.com/r/{subreddit}/search.json"
                params = {
                    'q': search_query,
                    'restrict_sr': 'true',
                    'limit': 5,
                    'sort': 'new',
                    't': 'month'
                }
                
                try:
                    response = self.session.get(url, params=params, timeout=10)
                    if response.status_code == 200:
                        data = response.json()
                        posts = data.get('data', {}).get('children', [])
                        print(f"  ‚ÑπÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –≤ r/{subreddit}")
                        
                        for post in posts:
                            post_data = post.get('data', {})
                            title = post_data.get('title', '')
                            selftext = post_data.get('selftext', '')
                            
                            # –£–ø—Ä–æ—Å—Ç–∏–º –ø—Ä–æ–≤–µ—Ä–∫—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                            all_results.append({
                                'title': title,
                                'subreddit': post_data.get('subreddit', ''),
                                'score': post_data.get('score', 0),
                                'url': f"https://reddit.com{post_data.get('permalink', '')}",
                                'author': post_data.get('author', ''),
                                'text': selftext[:300] if selftext else '',
                                'created': post_data.get('created_utc', 0)
                            })
                    else:
                        print(f"  ‚ùå –û—à–∏–±–∫–∞ {response.status_code} –¥–ª—è r/{subreddit}")
                except Exception as e:
                    print(f"  ‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è r/{subreddit}: {e}")
                    continue
            
            print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
            all_results.sort(key=lambda x: x['score'], reverse=True)
            return all_results[:5]
            
        except Exception as e:
            return [{'error': f'–û—à–∏–±–∫–∞ –æ–±—â–µ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e)}'}]
    
    def search_google(self, query):
        """–ü–æ–∏—Å–∫ –≤ Google (–æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ experimental-–≤–µ—Ä—Å–∏–∏).
        
        –ü—Ä–∏—á–∏–Ω–∞:
        - HTML-—Å–∫—Ä–µ–π–ø–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω –∏ –º–æ–∂–µ—Ç –Ω–∞—Ä—É—à–∞—Ç—å —É—Å–ª–æ–≤–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
        - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ WorkingWebAgent (DuckDuckGo + RSS) –∏–ª–∏ –≤–Ω–µ—à–Ω–∏–µ API (–Ω–∞–ø—Ä., SerpAPI/Tavily).
        """
        return [{'error': 'Google HTML scraping is disabled in experimental agent'}]
    
    def execute(self, task):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫"""
        print(f"üîç –í—ã–ø–æ–ª–Ω—è—é –ø–æ–∏—Å–∫: {task}")
        
        results = []
        
        # –ü–æ–∏—Å–∫ –≤ Reddit
        reddit_results = self.search_reddit(task)
        results.extend(reddit_results)

        # –û–±—â–∏–π –ø–æ–∏—Å–∫
        general_results = self.search_general(task)
        results.extend(general_results)

        # –ü–æ–∏—Å–∫ –≤ Google
        google_results = self.search_google(task)
        results.extend(google_results)
        
        return {
            'agent': 'Web Research Agent',
            'task': task,
            'results': results,
            'count': len(results),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
