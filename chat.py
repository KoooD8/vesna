#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç —Å AI –∞–≥–µ–Ω—Ç–∞–º–∏
"""

import sys
import os
import argparse
import logging
import json
from datetime import datetime
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–∞–ø–∫—É –≤ –ø—É—Ç—å Python
sys.path.append(os.path.dirname(__file__))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env, –µ—Å–ª–∏ –µ—Å—Ç—å
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

try:
    # —Ç–µ–ø–µ—Ä—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∏–∑ –ø–∞–∫–µ—Ç–∞
    from agents.web_research import WorkingWebAgent
    web_agent = None  # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∑–∂–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ CLI
    AGENT_IMPORT_OK = True
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ Working Web Research Agent: {e}")
    web_agent = None
    AGENT_IMPORT_OK = False

def print_results(result):
    """–ö—Ä–∞—Å–∏–≤–æ –ø–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–µ–º–∞)"""
    print(f"\nü§ñ –ê–≥–µ–Ω—Ç: {result.get('agent','Web Agent')}")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {result.get('count',0)}")
    print(f"‚è∞ –í—Ä–µ–º—è: {result.get('timestamp','')}")
    
    items = result.get('results') or []
    if items:
        print("\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        for i, item in enumerate(items, 1):
            if 'error' in item:
                print(f"{i}. ‚ùå {item['error']}")
                continue

            title = item.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            url = item.get('url', '')
            source = item.get('source', 'Unknown')
            snippet = (item.get('snippet') or '')[:200]
            meta = item.get('metadata') or {}

            print(f"\n{i}. [{source}] üìù {title}")
            if snippet:
                print(f"   üìù {snippet}...")
            # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            if source == 'Reddit':
                print(f"   üìç r/{meta.get('subreddit','unknown')} | üë§ u/{meta.get('author','unknown')}")
                print(f"   ‚¨ÜÔ∏è {meta.get('score',0)} –æ—á–∫–æ–≤")
            if source == 'Google News' and meta.get('date'):
                print(f"   üìÖ {meta.get('date')}")
            if url:
                print(f"   üîó {url}")
    else:
        print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

def build_parser():
    p = argparse.ArgumentParser(description="AI Agents Chat (Web Research)")
    p.add_argument("-v", "--verbose", action="store_true", help="–ø–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ (DEBUG)")
    p.add_argument("--health", action="store_true", help="–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–æ–≤ –∏ –≤—ã–π—Ç–∏")
    p.add_argument("--timeout", type=float, default=10.0, help="—Ç–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–æ–≤, —Å–µ–∫")
    p.add_argument("--max-results", type=int, default=5, help="–º–∞–∫—Å. —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫")
    p.add_argument("--retries", type=int, default=2, help="—á–∏—Å–ª–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö")
    p.add_argument("--backoff", type=float, default=0.5, help="–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç backoff –º–µ–∂–¥—É –ø–æ–≤—Ç–æ—Ä–∞–º–∏")

    sub = p.add_subparsers(dest="command")

    # list-steps: –ø–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —à–∞–≥–∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞
    sp_ls = sub.add_parser("list-steps", help="–ø–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —à–∞–≥–∏ pipeline (Registry)")

    # –û–±—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –≤—ã–±–æ—Ä–æ–∫/–ø–µ—á–∞—Ç–∏ –∏ –¥–ª—è vector search
    for sp in [p]:
        sp.add_argument("--filter-source", choices=["DuckDuckGo", "Reddit", "Google News", "test", "obsidian_md"], help="—Ñ–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É")
        sp.add_argument("--filter-domain", help="—Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–æ–º–µ–Ω—É (example.com)")
        sp.add_argument("--filter-date", help="—Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ YYYY-MM-DD (–¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π/–∏–Ω–≥–µ—Å—Ç–∞)")

    # search:web
    sp_web = sub.add_parser("search:web", help="–≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤–µ–±-–ø–æ–∏—Å–∫ (DDG/Reddit/RSS)")
    sp_web.add_argument("query", nargs="+", help="–∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞")
    sp_web.add_argument("--save", action="store_true", help="—Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Obsidian Sources")

    # search:news
    sp_news = sub.add_parser("search:news", help="–ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (Google News RSS)")
    sp_news.add_argument("query", nargs="+", help="–∑–∞–ø—Ä–æ—Å –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π")
    sp_news.add_argument("--save", action="store_true", help="—Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Obsidian Sources")

    # index:results
    sp_index = sub.add_parser("index:results", help="—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Obsidian Index")
    sp_index.add_argument("json_file", help="–ø—É—Ç—å –∫ JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ execute()")
    sp_index.add_argument("--name", default=None, help="–∏–º—è —Ñ–∞–π–ª–∞ –∏–Ω–¥–µ–∫—Å–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è")

    # summarize:file
    sp_sum = sub.add_parser("summarize:file", help="—Å–æ–∑–¥–∞—Ç—å –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –∏–∑ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")
    sp_sum.add_argument("json_file", help="–ø—É—Ç—å –∫ JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ execute()")
    sp_sum.add_argument("--title", default=None, help="–∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–º–µ—Ç–∫–∏ –≤ Summaries")

    # summarize:topk ‚Äî –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –∑–∞–ø–∏—Å—å —Ç–æ–ø-K –≤ Summaries
    sp_topk = sub.add_parser("summarize:topk", help="–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ Qdrant –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Top-K –≤ Summaries")
    sp_topk.add_argument("query", nargs="+", help="—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞")
    sp_topk.add_argument("--k", type=int, default=10, help="–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    sp_topk.add_argument("--title", default=None, help="–∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–º–µ—Ç–∫–∏")

    return p

def load_vault_config():
    from config import load_config
    cfg = load_config()
    return {"vault_path": cfg.vault_path, "folders": {
        "sources": cfg.folders.sources,
        "summaries": cfg.folders.summaries,
        "entities": cfg.folders.entities,
        "index": cfg.folders.index,
        "logs": cfg.folders.logs,
    }}

def save_note(base_dir: Path, folder: str, name: str, content: str):
    target_dir = base_dir / folder
    target_dir.mkdir(parents=True, exist_ok=True)
    path = target_dir / f"{name}.md"
    path.write_text(content, encoding="utf-8")
    print(f"üìù Saved: {path}")

def save_json(base_dir: Path, folder: str, name: str, obj: dict):
    target_dir = base_dir / folder
    target_dir.mkdir(parents=True, exist_ok=True)
    path = target_dir / f"{name}.json"
    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"üóÇÔ∏è Saved JSON: {path}")

def make_results_markdown(result: dict, title: str) -> str:
    lines = [f"# {title}", "", f"- Agent: {result.get('agent')}", f"- Time: {result.get('timestamp')}",""]
    for i, item in enumerate(result.get('results') or [], 1):
        if 'error' in item:
            lines.append(f"{i}. ‚ùå {item['error']}")
            continue
        lines.append(f"{i}. [{item.get('source','Unknown')}] {item.get('title','–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
        if item.get('url'):
            lines.append(f"   - URL: {item['url']}")
        if item.get('snippet'):
            lines.append(f"   - Snippet: {item['snippet'][:300]}...")
        meta = item.get('metadata') or {}
        if meta:
            lines.append(f"   - Meta: {json.dumps(meta, ensure_ascii=False)}")
    return "\n".join(lines)

def run_interactive(args):
    print("ü§ñ AI Agents Chat")
    print("=" * 30)
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
    print("‚Ä¢ help - –ø–æ–º–æ—â—å")
    print("‚Ä¢ quit - –≤—ã—Ö–æ–¥")
    print("‚Ä¢ –õ—é–±–æ–π —Ç–µ–∫—Å—Ç - –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Web Research Agent")
    while True:
        try:
            user_input = input("\nüí¨ –í–∞—à –∑–∞–ø—Ä–æ—Å: ").strip()
            if not user_input:
                continue
            if user_input.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            if user_input.lower() == 'help':
                print("\nüìö –°–ø—Ä–∞–≤–∫–∞:")
                print("‚Ä¢ –ö–æ–º–∞–Ω–¥—ã CLI: search:web, search:news, index:results, summarize:file")
                continue
            if web_agent:
                result = web_agent.execute(user_input)
                print_results(result)
            else:
                print("‚ùå Web Research Agent –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        except KeyboardInterrupt:
            print("\nüëã –í—ã—Ö–æ–¥")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

def main():
    parser = build_parser()
    args = parser.parse_args()

    # –ï—Å–ª–∏ –∫–æ–º–∞–Ω–¥–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞ ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å –ø–æ–º–æ—â—å –∏ —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
    if not args.health and args.command is None:
        available = ["search:web", "search:news", "index:results", "summarize:file", "summarize:topk"]
        print("‚ùì –ù–µ —É–∫–∞–∑–∞–Ω–∞ –∫–æ–º–∞–Ω–¥–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
        for c in available:
            print(f"  - {c}")
        parser.print_help()
        sys.exit(2)

    # –±–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–æ–≤ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ JSON –µ—Å–ª–∏ AI_STACK_JSON_LOGS=1)
    level = logging.DEBUG if args.verbose else logging.INFO
    if os.environ.get("AI_STACK_JSON_LOGS", "0") == "1":
        from logging_setup import setup_json_logger
        setup_json_logger(level)
    else:
        logging.basicConfig(level=level, format="%(asctime)s %(levelname)s %(name)s: %(message)s")
    logging.getLogger("urllib3").setLevel(logging.WARNING)

    global web_agent
    if AGENT_IMPORT_OK:
        try:
            web_agent = WorkingWebAgent(
                timeout=args.timeout,
                max_results=args.max_results,
                retries=args.retries,
                backoff=args.backoff,
                verbose=args.verbose
            )
        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç: {e}")
            web_agent = None
    else:
        print("‚ùå Web Research Agent –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–∏–º–ø–æ—Ä—Ç –Ω–µ —É–¥–∞–ª—Å—è)")

    # health-check —Ä–µ–∂–∏–º: –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
    if args.health:
        ok = True
        # –ü—Ä–æ–≤–µ—Ä–∏–º –∏–º–ø–æ—Ä—Ç –∞–≥–µ–Ω—Ç–∞
        if not AGENT_IMPORT_OK:
            print("‚ùå Web Research Agent –∏–º–ø–æ—Ä—Ç: FAIL")
            ok = False
        else:
            print("‚úÖ Web Research Agent –∏–º–ø–æ—Ä—Ç: OK")
        # –ü—Ä–æ–≤–µ—Ä–∏–º –∫–æ–Ω—Ñ–∏–≥ Vault
        cfg = load_vault_config()
        base = Path(cfg["vault_path"])
        try:
            base.mkdir(parents=True, exist_ok=True)
            print(f"‚úÖ Vault –ø—É—Ç—å: {base} –¥–æ—Å—Ç—É–ø–µ–Ω")
        except Exception as e:
            print(f"‚ùå Vault –ø—É—Ç—å –æ—à–∏–±–∫–∞: {e}")
            ok = False
        # –ü—Ä–æ–≤–µ—Ä–∏–º Qdrant readyz (–ª–æ–∫–∞–ª—å–Ω–æ)
        try:
            import urllib.request
            with urllib.request.urlopen(os.environ.get("AI_STACK_QDRANT_URL", "http://localhost:6333") + "/readyz", timeout=2) as resp:
                ready = resp.read().decode("utf-8").strip()
                print("‚úÖ Qdrant readyz:", ready)
        except Exception as e:
            print(f"‚ö†Ô∏è  Qdrant readyz –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª—ë–≥–∫–æ–≥–æ vector search –∑–∞–ø—Ä–æ—Å–∞ (–µ—Å–ª–∏ Qdrant –ø–æ–¥–Ω—è—Ç)
        try:
            from vector_store import VectorStore
            vs = VectorStore()
            _ = vs.search("health-check", limit=1)
            print("‚úÖ Vector search: OK")
        except Exception as e:
            print(f"‚ö†Ô∏è  Vector search –æ—à–∏–±–∫–∞: {e}")
        print("OK" if ok else "FAIL")
        return 0
    # –µ—Å–ª–∏ –Ω–µ—Ç —Å–∞–±–∫–æ–º–∞–Ω–¥ ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤
    if not args.command:
        return run_interactive(args)

    # –∑–∞–≥—Ä—É–∑–∏–º –∫–æ–Ω—Ñ–∏–≥ Vault
    cfg = load_vault_config()
    base = Path(cfg["vault_path"])
    folders = cfg["folders"]

    # –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∞–±–∫–æ–º–∞–Ω–¥
    def _apply_filters(obj: dict) -> dict:
        items = obj.get("results") or []
        src = getattr(args, "filter_source", None)
        dom = getattr(args, "filter_domain", None)
        fdate = getattr(args, "filter_date", None)
        if src:
            items = [r for r in items if r.get("source") == src]
        if dom:
            def _match_domain(u: str) -> bool:
                try:
                    from urllib.parse import urlparse
                    net = urlparse(u or "").netloc
                    return net.endswith(dom)
                except Exception:
                    return False
            items = [r for r in items if _match_domain(r.get("url", ""))]
        if fdate:
            # –û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏, –≥–¥–µ metadata.date >= YYYY-MM-DD (–ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
            items = [r for r in items if (r.get("metadata") or {}).get("date","")[:10] >= fdate]
        obj["results"] = items
        obj["count"] = len(items)
        return obj

    if args.command == "search:web":
        query = " ".join(args.query)
        result = web_agent.execute(query) if web_agent else {"results": [], "agent":"n/a","timestamp":datetime.now().isoformat(),"count":0}
        result = _apply_filters(result)
        print_results(result)
        if getattr(args, "save", False):
            ts = datetime.now().strftime("%Y%m%d-%H%M%S")
            name = f"search-web-{ts}"
            save_json(base, folders["index"], name, result)
            md = make_results_markdown(result, f"Search Web: {query}")
            save_note(base, folders["sources"], name, md)

    elif args.command == "search:news":
        query = " ".join(args.query)
        result = web_agent.execute(query) if web_agent else {"results": [], "agent":"n/a","timestamp":datetime.now().isoformat(),"count":0}
        result["results"] = [r for r in (result.get("results") or []) if r.get("source") == "Google News"]
        result = _apply_filters(result)
        print_results(result)
        if getattr(args, "save", False):
            ts = datetime.now().strftime("%Y%m%d-%H%M%S")
            name = f"search-news-{ts}"
            save_json(base, folders["index"], name, result)
            md = make_results_markdown(result, f"Search News: {query}")
            save_note(base, folders["sources"], name, md)

    elif args.command == "index:results":
        jf = Path(args.json_file)
        if not jf.exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {jf}")
            return
        with open(jf, "r", encoding="utf-8") as f:
            obj = json.load(f)
        name = args.name or jf.stem
        save_json(base, folders["index"], name, obj)
        print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
 
    elif args.command == "summarize:file":
        jf = Path(args.json_file)
        if not jf.exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {jf}")
            return
        with open(jf, "r", encoding="utf-8") as f:
            obj = json.load(f)
        title = args.title or f"Summary {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        md = make_results_markdown(obj, title)
        ts = datetime.now().strftime("%Y%m%d-%H%M%S")
        name = f"summary-{ts}"
        save_note(base, folders["summaries"], name, md)
        print("‚úÖ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
 
    elif args.command == "list-steps":
        from orchestrator.registry import Registry
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —à–∞–≥–∏ (Registry):")
        for k in sorted(Registry.keys()):
            print(f" - {k}")

    elif args.command == "summarize:topk":
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant —Å server-side —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ –∏ –∑–∞–ø–∏—Å—å—é –≤ Summaries
        from vector_store import VectorStore
        query = " ".join(args.query)
        vs = VectorStore()
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        src = getattr(args, "filter_source", None)
        dom = getattr(args, "filter_domain", None)
        fdate = getattr(args, "filter_date", None)
        k = getattr(args, "k", 10)
        points = vs.search(query, limit=k, source=src, domain=dom, date_from=fdate)
        # –†–µ–Ω–¥–µ—Ä Markdown
        lines = [f"# Top-{k} Vector Search: {query}", ""]
        for i, p in enumerate(points, 1):
            payload = p.payload if isinstance(p, dict) else getattr(p, "payload", {})  # –ø–æ–¥—Å—Ç—Ä–∞—Ö—É–µ–º—Å—è –ø–æ–¥ –º–æ–∫
            score = p.get("score") if isinstance(p, dict) else getattr(p, "score", 0.0)
            title = payload.get("title") or payload.get("file") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            url = payload.get("url") or ""
            src2 = payload.get("source") or ""
            lines.append(f"{i}. [{src2}] {title} ‚Äî score: {round(float(score), 4)}")
            if url:
                lines.append(f"   - URL: {url}")
        md = "\n".join(lines)
        ts = datetime.now().strftime("%Y%m%d-%H%M%S")
        name = f"vector-topk-{ts}"
        title_opt = getattr(args, "title", None)
        if title_opt:
            md = "# " + title_opt + "\n\n" + "\n".join(lines[1:])
        save_note(base, folders["summaries"], name, md)
        print("‚úÖ Top-K —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Summaries")
 

if __name__ == "__main__":
    main()
